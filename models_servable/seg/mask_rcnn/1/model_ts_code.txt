
Code for root model, type=TracingAdapter:
class TracingAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  model : __torch__.detectron2.modeling.meta_arch.rcnn.GeneralizedRCNN
  def forward(self: __torch__.detectron2.export.flatten.TracingAdapter,
    argument_1: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    model = self.model
    roi_heads = model.roi_heads
    model0 = self.model
    proposal_generator = model0.proposal_generator
    model1 = self.model
    backbone = model1.backbone
    model2 = self.model
    pixel_std = model2.pixel_std
    model3 = self.model
    pixel_mean = model3.pixel_mean
    x = _0(argument_1, pixel_mean, )
    t = torch.div(torch.sub(x, pixel_mean), pixel_std)
    _1 = ops.prim.NumToTensor(torch.size(t, 1))
    _2 = ops.prim.NumToTensor(torch.size(t, 2))
    image_size = torch.to(torch.stack([_1, _2]), dtype=4, layout=0, device=torch.device("cuda"))
    _3 = torch.to(torch.stack([image_size]), dtype=4, layout=0, device=torch.device("cuda"))
    max_size, _4 = torch.max(_3, 0)
    _5 = torch.div(torch.add(max_size, CONSTANTS.c0), CONSTANTS.c1, rounding_mode="floor")
    max_size0 = torch.mul(_5, CONSTANTS.c1)
    _6 = torch.sub(torch.select(max_size0, 0, -1), torch.select(image_size, 0, 1))
    _7 = int(_6)
    _8 = torch.sub(torch.select(max_size0, 0, -2), torch.select(image_size, 0, 0))
    _9 = torch.constant_pad_nd(t, [0, _7, 0, int(_8)], 0.)
    batched_imgs = torch.unsqueeze_(_9, 0)
    x0 = torch.contiguous(batched_imgs)
    _10, _11, _12, _13, _14, _15, _16, _17, _18, _19, _20, = (backbone).forward(x0, )
    _21 = (proposal_generator).forward(_10, _11, _12, _13, _14, _15, _16, _17, _18, image_size, )
    _22 = (roi_heads).forward(_10, _21, _11, _12, _19, image_size, _20, )
    _23, _24, _25, _26, = _22
    return (_23, _24, _25, _26, image_size)

--------------------------------------------------------------------------------
Code for .model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  backbone : __torch__.detectron2.modeling.backbone.fpn.FPN
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads

--------------------------------------------------------------------------------
Code for .model.backbone, type=FPN:
class FPN(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fpn_lateral2 : __torch__.detectron2.layers.wrappers.Conv2d
  fpn_output2 : __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d
  fpn_lateral3 : __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d
  fpn_output3 : __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d
  fpn_lateral4 : __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d
  fpn_output4 : __torch__.detectron2.layers.wrappers.___torch_mangle_4.Conv2d
  fpn_lateral5 : __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d
  fpn_output5 : __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d
  top_block : __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool
  bottom_up : __torch__.detectron2.modeling.backbone.resnet.ResNet
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.FPN,
    x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    top_block = self.top_block
    fpn_output2 = self.fpn_output2
    fpn_lateral2 = self.fpn_lateral2
    fpn_output3 = self.fpn_output3
    fpn_lateral3 = self.fpn_lateral3
    fpn_output4 = self.fpn_output4
    fpn_lateral4 = self.fpn_lateral4
    fpn_output5 = self.fpn_output5
    fpn_lateral5 = self.fpn_lateral5
    bottom_up = self.bottom_up
    _0, _1, _2, _3, = (bottom_up).forward(x, )
    _4 = (fpn_lateral5).forward(_0, )
    _5 = (fpn_output5).forward(_4, )
    top_down_features = torch.upsample_nearest2d(_4, None, [2., 2.])
    x0 = torch.add((fpn_lateral4).forward(_1, ), top_down_features)
    _6 = (fpn_output4).forward(x0, )
    top_down_features0 = torch.upsample_nearest2d(x0, None, [2., 2.])
    x1 = torch.add((fpn_lateral3).forward(_2, ), top_down_features0)
    _7 = (fpn_output3).forward(x1, )
    top_down_features1 = torch.upsample_nearest2d(x1, None, [2., 2.])
    x2 = torch.add((fpn_lateral2).forward(_3, ), top_down_features1)
    _8 = ((fpn_output2).forward(x2, ), _7, _6, _5, _5, _5, _5, (top_block).forward(_5, ), _5, _5, _5)
    return _8

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    lateral_features = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_0.Conv2d,
    x: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    feature_map = torch._convolution(x, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_1.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    lateral_features = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_2.Conv2d,
    x: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    feature_map = torch._convolution(x, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_3.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    lateral_features = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return lateral_features

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_4.Conv2d,
    x: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    feature_map = torch._convolution(x, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_lateral5, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_5.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x

--------------------------------------------------------------------------------
Code for .model.backbone.fpn_output5, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_6.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return input

--------------------------------------------------------------------------------
Code for .model.backbone.top_block, type=LastLevelMaxPool:
class LastLevelMaxPool(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.modeling.backbone.fpn.LastLevelMaxPool,
    argument_1: Tensor) -> Tensor:
    feature_map = torch.max_pool2d(argument_1, [1, 1], [2, 2], [0, 0], [1, 1])
    return feature_map

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up, type=ResNet:
class ResNet(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stem : __torch__.detectron2.modeling.backbone.resnet.BasicStem
  res2 : __torch__.torch.nn.modules.container.Sequential
  res3 : __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential
  res4 : __torch__.torch.nn.modules.container.___torch_mangle_105.Sequential
  res5 : __torch__.torch.nn.modules.container.___torch_mangle_129.Sequential
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.ResNet,
    x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    res5 = self.res5
    res4 = self.res4
    res3 = self.res3
    res2 = self.res2
    stem = self.stem
    _0 = (res2).forward((stem).forward(x, ), )
    _1 = (res3).forward(_0, )
    _2 = (res4).forward(_1, )
    return ((res5).forward(_2, ), _2, _1, _0)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem, type=BasicStem:
class BasicStem(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BasicStem,
    x: Tensor) -> Tensor:
    conv1 = self.conv1
    input = torch.relu_((conv1).forward(x, ))
    x0 = torch.max_pool2d(input, [3, 3], [2, 2], [1, 1], [1, 1])
    return x0

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_7.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.stem.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    x = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return x

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_22.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_29.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_9.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_11.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    shortcut = self.shortcut
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), (shortcut).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_8.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_9.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_8.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    shortcut = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_10.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_11.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_10.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_12.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_13.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_12.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_14.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_15.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_14.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_17.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_19.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_22.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_16.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_17.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_16.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_18.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_19.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_18.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_20.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_21.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_20.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_28.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_29.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_23.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_24.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_23.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_25.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_26.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_25.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_27.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_28.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res2.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_27.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_38.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_45.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_52.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_59.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential,
    argument_1: Tensor) -> Tensor:
    _3 = getattr(self, "3")
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _4 = (_1).forward((_0).forward(argument_1, ), )
    return (_3).forward((_2).forward(_4, ), )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_33.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_35.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_37.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_38.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    shortcut = self.shortcut
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), (shortcut).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_30.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_31.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_30.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    shortcut = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_32.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_33.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_32.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_34.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_35.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_34.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_36.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_37.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_36.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_40.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_44.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_45.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_39.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_40.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_39.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_41.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_42.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_41.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_43.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_44.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_43.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_47.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_49.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_51.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_52.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_46.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_47.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_46.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_48.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_49.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_48.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_50.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_51.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_50.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_54.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_56.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_58.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_59.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_53.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_54.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_53.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_55.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_56.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_55.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_57.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_58.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res3.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_57.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_69.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_76.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_83.BottleneckBlock
  __annotations__["3"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_90.BottleneckBlock
  __annotations__["4"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_97.BottleneckBlock
  __annotations__["5"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_104.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_105.Sequential,
    argument_1: Tensor) -> Tensor:
    _5 = getattr(self, "5")
    _4 = getattr(self, "4")
    _3 = getattr(self, "3")
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _6 = (_1).forward((_0).forward(argument_1, ), )
    _7 = (_4).forward((_3).forward((_2).forward(_6, ), ), )
    return (_5).forward(_7, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_62.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_64.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_66.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_68.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_69.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    shortcut = self.shortcut
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), (shortcut).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_61.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_62.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_61.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    shortcut = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_63.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_64.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_63.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_65.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_66.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_65.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_67.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_68.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_67.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_71.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_73.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_75.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_76.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_70.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_71.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_70.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_72.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_73.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_72.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_74.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_75.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_74.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_78.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_80.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_82.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_83.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_77.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_78.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_77.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_79.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_80.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_79.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_81.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_82.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_81.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_85.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_87.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_89.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_90.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_84.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_85.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_84.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_86.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_87.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_86.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_88.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_89.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.3.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_88.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_92.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_94.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_96.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_97.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_91.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_92.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_91.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_93.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_94.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_93.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_95.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_96.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.4.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_95.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_99.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_101.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_103.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_104.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_98.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_99.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_98.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_100.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_101.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_100.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_102.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_103.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res4.5.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_102.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_114.BottleneckBlock
  __annotations__["1"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_121.BottleneckBlock
  __annotations__["2"] = __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_128.BottleneckBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_129.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  shortcut : __torch__.detectron2.layers.wrappers.___torch_mangle_107.Conv2d
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_109.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_111.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_113.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_114.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    shortcut = self.shortcut
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), (shortcut).forward(argument_1, ))
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.shortcut, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_106.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_107.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.shortcut.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_106.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    shortcut = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return shortcut

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_108.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_109.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_108.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_110.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_111.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_110.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_112.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_113.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.0.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_112.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_116.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_118.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_120.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_121.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_115.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_116.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_115.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_117.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_118.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_117.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_119.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_120.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.1.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_119.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2, type=BottleneckBlock:
class BottleneckBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv1 : __torch__.detectron2.layers.wrappers.___torch_mangle_123.Conv2d
  conv2 : __torch__.detectron2.layers.wrappers.___torch_mangle_125.Conv2d
  conv3 : __torch__.detectron2.layers.wrappers.___torch_mangle_127.Conv2d
  def forward(self: __torch__.detectron2.modeling.backbone.resnet.___torch_mangle_128.BottleneckBlock,
    argument_1: Tensor) -> Tensor:
    conv3 = self.conv3
    conv2 = self.conv2
    conv1 = self.conv1
    x = torch.relu_((conv1).forward(argument_1, ))
    x0 = torch.relu_((conv2).forward(x, ))
    out = torch.add_((conv3).forward(x0, ), argument_1)
    return torch.relu_(out)

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_122.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_123.Conv2d,
    argument_1: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(argument_1, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv1.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_122.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_124.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_125.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv2.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_124.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", ]
  __buffers__ = []
  weight : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  norm : __torch__.detectron2.layers.batch_norm.___torch_mangle_126.FrozenBatchNorm2d
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_127.Conv2d,
    x: Tensor) -> Tensor:
    norm = self.norm
    weight = self.weight
    input = torch._convolution(x, weight, None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (norm).forward(input, )

--------------------------------------------------------------------------------
Code for .model.backbone.bottom_up.res5.2.conv3.norm, type=FrozenBatchNorm2d:
class FrozenBatchNorm2d(Module):
  __parameters__ = []
  __buffers__ = ["weight", "bias", "running_mean", "running_var", ]
  weight : Tensor
  bias : Tensor
  running_mean : Tensor
  running_var : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.batch_norm.___torch_mangle_126.FrozenBatchNorm2d,
    input: Tensor) -> Tensor:
    running_var = self.running_var
    running_mean = self.running_mean
    bias = self.bias
    weight = self.weight
    out = torch.batch_norm(input, weight, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)
    return out

--------------------------------------------------------------------------------
Code for .model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  rpn_head : __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.RPN,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    argument_6: Tensor,
    argument_7: Tensor,
    argument_8: Tensor,
    argument_9: Tensor,
    image_size: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    _2 = __torch__.detectron2.layers.wrappers.move_device_like
    _3 = __torch__.detectron2.layers.wrappers.move_device_like
    _4 = __torch__.detectron2.layers.wrappers.move_device_like
    _5 = __torch__.detectron2.layers.wrappers.move_device_like
    _6 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    rpn_head = self.rpn_head
    anchor_generator = self.anchor_generator
    _7 = (anchor_generator).forward(argument_1, argument_2, argument_3, argument_4, argument_5, argument_6, argument_7, argument_8, )
    _8, _9, _10, _11, _12, = _7
    _13 = (rpn_head).forward(argument_1, argument_2, argument_3, argument_9, argument_8, )
    _14, _15, _16, _17, _18, _19, _20, _21, _22, _23, = _13
    logits_i = torch.flatten(torch.permute(_14, [0, 2, 3, 1]), 1)
    logits_i0 = torch.flatten(torch.permute(_15, [0, 2, 3, 1]), 1)
    logits_i1 = torch.flatten(torch.permute(_16, [0, 2, 3, 1]), 1)
    logits_i2 = torch.flatten(torch.permute(_17, [0, 2, 3, 1]), 1)
    logits_i3 = torch.flatten(torch.permute(_18, [0, 2, 3, 1]), 1)
    _24 = ops.prim.NumToTensor(torch.size(_19, 0))
    _25 = int(_24)
    _26 = ops.prim.NumToTensor(torch.size(_19, 2))
    _27 = int(_26)
    _28 = ops.prim.NumToTensor(torch.size(_19, 3))
    _29 = torch.view(_19, [_25, -1, 4, _27, int(_28)])
    pred_anchor_deltas_i = torch.flatten(torch.permute(_29, [0, 3, 4, 1, 2]), 1, -2)
    _30 = ops.prim.NumToTensor(torch.size(_20, 0))
    _31 = int(_30)
    _32 = ops.prim.NumToTensor(torch.size(_20, 2))
    _33 = int(_32)
    _34 = ops.prim.NumToTensor(torch.size(_20, 3))
    _35 = torch.view(_20, [_31, -1, 4, _33, int(_34)])
    pred_anchor_deltas_i0 = torch.flatten(torch.permute(_35, [0, 3, 4, 1, 2]), 1, -2)
    _36 = ops.prim.NumToTensor(torch.size(_21, 0))
    _37 = int(_36)
    _38 = ops.prim.NumToTensor(torch.size(_21, 2))
    _39 = int(_38)
    _40 = ops.prim.NumToTensor(torch.size(_21, 3))
    _41 = torch.view(_21, [_37, -1, 4, _39, int(_40)])
    pred_anchor_deltas_i1 = torch.flatten(torch.permute(_41, [0, 3, 4, 1, 2]), 1, -2)
    _42 = ops.prim.NumToTensor(torch.size(_22, 0))
    _43 = int(_42)
    _44 = ops.prim.NumToTensor(torch.size(_22, 2))
    _45 = int(_44)
    _46 = ops.prim.NumToTensor(torch.size(_22, 3))
    _47 = torch.view(_22, [_43, -1, 4, _45, int(_46)])
    pred_anchor_deltas_i2 = torch.flatten(torch.permute(_47, [0, 3, 4, 1, 2]), 1, -2)
    _48 = ops.prim.NumToTensor(torch.size(_23, 0))
    _49 = int(_48)
    _50 = ops.prim.NumToTensor(torch.size(_23, 2))
    _51 = int(_50)
    _52 = ops.prim.NumToTensor(torch.size(_23, 3))
    _53 = torch.view(_23, [_49, -1, 4, _51, int(_52)])
    pred_anchor_deltas_i3 = torch.flatten(torch.permute(_53, [0, 3, 4, 1, 2]), 1, -2)
    N = ops.prim.NumToTensor(torch.size(pred_anchor_deltas_i, 0))
    _54 = int(N)
    _55 = int(N)
    _56 = int(N)
    _57 = int(N)
    _58 = int(N)
    _59 = int(N)
    _60 = int(N)
    _61 = int(N)
    _62 = int(N)
    _63 = int(N)
    B = ops.prim.NumToTensor(torch.size(_8, 1))
    _64 = int(B)
    _65 = int(B)
    deltas = torch.reshape(pred_anchor_deltas_i, [-1, int(B)])
    _66 = torch.expand(torch.unsqueeze(_8, 0), [_63, -1, -1])
    boxes = torch.reshape(_66, [-1, _65])
    deltas0 = torch.to(deltas, 6)
    boxes0 = torch.to(boxes, 6)
    _67 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _68 = torch.select(_67, 1, 2)
    _69 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    widths = torch.sub(_68, torch.select(_69, 1, 0))
    _70 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _71 = torch.select(_70, 1, 3)
    _72 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    heights = torch.sub(_71, torch.select(_72, 1, 1))
    _73 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_73, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _74 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_74, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _75 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _76 = torch.slice(_75, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_76, CONSTANTS.c1)
    _77 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _78 = torch.slice(_77, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_78, CONSTANTS.c1)
    _79 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _80 = torch.slice(_79, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_80, CONSTANTS.c1)
    _81 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _82 = torch.slice(_81, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_82, CONSTANTS.c1)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _83 = torch.slice(widths, 0, 0, 9223372036854775807)
    _84 = torch.mul(dx, torch.unsqueeze(_83, 1))
    _85 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_84, torch.unsqueeze(_85, 1))
    _86 = torch.slice(heights, 0, 0, 9223372036854775807)
    _87 = torch.mul(dy, torch.unsqueeze(_86, 1))
    _88 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_87, torch.unsqueeze(_88, 1))
    _89 = torch.exp(dw0)
    _90 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_89, torch.unsqueeze(_90, 1))
    _91 = torch.exp(dh0)
    _92 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_91, torch.unsqueeze(_92, 1))
    _93 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    _94 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    _95 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    _96 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    _97 = torch.stack([_93, _94, _95, _96], -1)
    pred_boxes = torch.to(_97, dtype=6, layout=0, device=torch.device("cuda"))
    _98 = ops.prim.NumToTensor(torch.size(deltas0, 0))
    _99 = int(_98)
    _100 = ops.prim.NumToTensor(torch.size(deltas0, 1))
    proposals_i = torch.reshape(pred_boxes, [_99, int(_100)])
    proposals_i0 = torch.view(proposals_i, [_62, -1, _64])
    B0 = ops.prim.NumToTensor(torch.size(_9, 1))
    _101 = int(B0)
    _102 = int(B0)
    deltas1 = torch.reshape(pred_anchor_deltas_i0, [-1, int(B0)])
    _103 = torch.expand(torch.unsqueeze(_9, 0), [_61, -1, -1])
    boxes1 = torch.reshape(_103, [-1, _102])
    deltas2 = torch.to(deltas1, 6)
    boxes2 = torch.to(boxes1, 6)
    _104 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    _105 = torch.select(_104, 1, 2)
    _106 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    widths0 = torch.sub(_105, torch.select(_106, 1, 0))
    _107 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    _108 = torch.select(_107, 1, 3)
    _109 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    heights0 = torch.sub(_108, torch.select(_109, 1, 1))
    _110 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    ctr_x0 = torch.add(torch.select(_110, 1, 0), torch.mul(widths0, CONSTANTS.c0))
    _111 = torch.slice(boxes2, 0, 0, 9223372036854775807)
    ctr_y0 = torch.add(torch.select(_111, 1, 1), torch.mul(heights0, CONSTANTS.c0))
    _112 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _113 = torch.slice(_112, 1, 0, 9223372036854775807, 4)
    dx0 = torch.div(_113, CONSTANTS.c1)
    _114 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _115 = torch.slice(_114, 1, 1, 9223372036854775807, 4)
    dy0 = torch.div(_115, CONSTANTS.c1)
    _116 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _117 = torch.slice(_116, 1, 2, 9223372036854775807, 4)
    dw1 = torch.div(_117, CONSTANTS.c1)
    _118 = torch.slice(deltas2, 0, 0, 9223372036854775807)
    _119 = torch.slice(_118, 1, 3, 9223372036854775807, 4)
    dh1 = torch.div(_119, CONSTANTS.c1)
    dw2 = torch.clamp(dw1, None, 4.1351665567423561)
    dh2 = torch.clamp(dh1, None, 4.1351665567423561)
    _120 = torch.slice(widths0, 0, 0, 9223372036854775807)
    _121 = torch.mul(dx0, torch.unsqueeze(_120, 1))
    _122 = torch.slice(ctr_x0, 0, 0, 9223372036854775807)
    pred_ctr_x0 = torch.add(_121, torch.unsqueeze(_122, 1))
    _123 = torch.slice(heights0, 0, 0, 9223372036854775807)
    _124 = torch.mul(dy0, torch.unsqueeze(_123, 1))
    _125 = torch.slice(ctr_y0, 0, 0, 9223372036854775807)
    pred_ctr_y0 = torch.add(_124, torch.unsqueeze(_125, 1))
    _126 = torch.exp(dw2)
    _127 = torch.slice(widths0, 0, 0, 9223372036854775807)
    pred_w0 = torch.mul(_126, torch.unsqueeze(_127, 1))
    _128 = torch.exp(dh2)
    _129 = torch.slice(heights0, 0, 0, 9223372036854775807)
    pred_h0 = torch.mul(_128, torch.unsqueeze(_129, 1))
    _130 = torch.sub(pred_ctr_x0, torch.mul(pred_w0, CONSTANTS.c0))
    _131 = torch.sub(pred_ctr_y0, torch.mul(pred_h0, CONSTANTS.c0))
    _132 = torch.add(pred_ctr_x0, torch.mul(pred_w0, CONSTANTS.c0))
    _133 = torch.add(pred_ctr_y0, torch.mul(pred_h0, CONSTANTS.c0))
    _134 = torch.stack([_130, _131, _132, _133], -1)
    pred_boxes0 = torch.to(_134, dtype=6, layout=0, device=torch.device("cuda"))
    _135 = ops.prim.NumToTensor(torch.size(deltas2, 0))
    _136 = int(_135)
    _137 = ops.prim.NumToTensor(torch.size(deltas2, 1))
    proposals_i1 = torch.reshape(pred_boxes0, [_136, int(_137)])
    proposals_i2 = torch.view(proposals_i1, [_60, -1, _101])
    B1 = ops.prim.NumToTensor(torch.size(_10, 1))
    _138 = int(B1)
    _139 = int(B1)
    deltas3 = torch.reshape(pred_anchor_deltas_i1, [-1, int(B1)])
    _140 = torch.expand(torch.unsqueeze(_10, 0), [_59, -1, -1])
    boxes3 = torch.reshape(_140, [-1, _139])
    deltas4 = torch.to(deltas3, 6)
    boxes4 = torch.to(boxes3, 6)
    _141 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    _142 = torch.select(_141, 1, 2)
    _143 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    widths1 = torch.sub(_142, torch.select(_143, 1, 0))
    _144 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    _145 = torch.select(_144, 1, 3)
    _146 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    heights1 = torch.sub(_145, torch.select(_146, 1, 1))
    _147 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    ctr_x1 = torch.add(torch.select(_147, 1, 0), torch.mul(widths1, CONSTANTS.c0))
    _148 = torch.slice(boxes4, 0, 0, 9223372036854775807)
    ctr_y1 = torch.add(torch.select(_148, 1, 1), torch.mul(heights1, CONSTANTS.c0))
    _149 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _150 = torch.slice(_149, 1, 0, 9223372036854775807, 4)
    dx1 = torch.div(_150, CONSTANTS.c1)
    _151 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _152 = torch.slice(_151, 1, 1, 9223372036854775807, 4)
    dy1 = torch.div(_152, CONSTANTS.c1)
    _153 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _154 = torch.slice(_153, 1, 2, 9223372036854775807, 4)
    dw3 = torch.div(_154, CONSTANTS.c1)
    _155 = torch.slice(deltas4, 0, 0, 9223372036854775807)
    _156 = torch.slice(_155, 1, 3, 9223372036854775807, 4)
    dh3 = torch.div(_156, CONSTANTS.c1)
    dw4 = torch.clamp(dw3, None, 4.1351665567423561)
    dh4 = torch.clamp(dh3, None, 4.1351665567423561)
    _157 = torch.slice(widths1, 0, 0, 9223372036854775807)
    _158 = torch.mul(dx1, torch.unsqueeze(_157, 1))
    _159 = torch.slice(ctr_x1, 0, 0, 9223372036854775807)
    pred_ctr_x1 = torch.add(_158, torch.unsqueeze(_159, 1))
    _160 = torch.slice(heights1, 0, 0, 9223372036854775807)
    _161 = torch.mul(dy1, torch.unsqueeze(_160, 1))
    _162 = torch.slice(ctr_y1, 0, 0, 9223372036854775807)
    pred_ctr_y1 = torch.add(_161, torch.unsqueeze(_162, 1))
    _163 = torch.exp(dw4)
    _164 = torch.slice(widths1, 0, 0, 9223372036854775807)
    pred_w1 = torch.mul(_163, torch.unsqueeze(_164, 1))
    _165 = torch.exp(dh4)
    _166 = torch.slice(heights1, 0, 0, 9223372036854775807)
    pred_h1 = torch.mul(_165, torch.unsqueeze(_166, 1))
    _167 = torch.sub(pred_ctr_x1, torch.mul(pred_w1, CONSTANTS.c0))
    _168 = torch.sub(pred_ctr_y1, torch.mul(pred_h1, CONSTANTS.c0))
    _169 = torch.add(pred_ctr_x1, torch.mul(pred_w1, CONSTANTS.c0))
    _170 = torch.add(pred_ctr_y1, torch.mul(pred_h1, CONSTANTS.c0))
    _171 = torch.stack([_167, _168, _169, _170], -1)
    pred_boxes1 = torch.to(_171, dtype=6, layout=0, device=torch.device("cuda"))
    _172 = ops.prim.NumToTensor(torch.size(deltas4, 0))
    _173 = int(_172)
    _174 = ops.prim.NumToTensor(torch.size(deltas4, 1))
    proposals_i3 = torch.reshape(pred_boxes1, [_173, int(_174)])
    proposals_i4 = torch.view(proposals_i3, [_58, -1, _138])
    B2 = ops.prim.NumToTensor(torch.size(_11, 1))
    _175 = int(B2)
    _176 = int(B2)
    deltas5 = torch.reshape(pred_anchor_deltas_i2, [-1, int(B2)])
    _177 = torch.expand(torch.unsqueeze(_11, 0), [_57, -1, -1])
    boxes5 = torch.reshape(_177, [-1, _176])
    deltas6 = torch.to(deltas5, 6)
    boxes6 = torch.to(boxes5, 6)
    _178 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    _179 = torch.select(_178, 1, 2)
    _180 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    widths2 = torch.sub(_179, torch.select(_180, 1, 0))
    _181 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    _182 = torch.select(_181, 1, 3)
    _183 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    heights2 = torch.sub(_182, torch.select(_183, 1, 1))
    _184 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    ctr_x2 = torch.add(torch.select(_184, 1, 0), torch.mul(widths2, CONSTANTS.c0))
    _185 = torch.slice(boxes6, 0, 0, 9223372036854775807)
    ctr_y2 = torch.add(torch.select(_185, 1, 1), torch.mul(heights2, CONSTANTS.c0))
    _186 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _187 = torch.slice(_186, 1, 0, 9223372036854775807, 4)
    dx2 = torch.div(_187, CONSTANTS.c1)
    _188 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _189 = torch.slice(_188, 1, 1, 9223372036854775807, 4)
    dy2 = torch.div(_189, CONSTANTS.c1)
    _190 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _191 = torch.slice(_190, 1, 2, 9223372036854775807, 4)
    dw5 = torch.div(_191, CONSTANTS.c1)
    _192 = torch.slice(deltas6, 0, 0, 9223372036854775807)
    _193 = torch.slice(_192, 1, 3, 9223372036854775807, 4)
    dh5 = torch.div(_193, CONSTANTS.c1)
    dw6 = torch.clamp(dw5, None, 4.1351665567423561)
    dh6 = torch.clamp(dh5, None, 4.1351665567423561)
    _194 = torch.slice(widths2, 0, 0, 9223372036854775807)
    _195 = torch.mul(dx2, torch.unsqueeze(_194, 1))
    _196 = torch.slice(ctr_x2, 0, 0, 9223372036854775807)
    pred_ctr_x2 = torch.add(_195, torch.unsqueeze(_196, 1))
    _197 = torch.slice(heights2, 0, 0, 9223372036854775807)
    _198 = torch.mul(dy2, torch.unsqueeze(_197, 1))
    _199 = torch.slice(ctr_y2, 0, 0, 9223372036854775807)
    pred_ctr_y2 = torch.add(_198, torch.unsqueeze(_199, 1))
    _200 = torch.exp(dw6)
    _201 = torch.slice(widths2, 0, 0, 9223372036854775807)
    pred_w2 = torch.mul(_200, torch.unsqueeze(_201, 1))
    _202 = torch.exp(dh6)
    _203 = torch.slice(heights2, 0, 0, 9223372036854775807)
    pred_h2 = torch.mul(_202, torch.unsqueeze(_203, 1))
    _204 = torch.sub(pred_ctr_x2, torch.mul(pred_w2, CONSTANTS.c0))
    _205 = torch.sub(pred_ctr_y2, torch.mul(pred_h2, CONSTANTS.c0))
    _206 = torch.add(pred_ctr_x2, torch.mul(pred_w2, CONSTANTS.c0))
    _207 = torch.add(pred_ctr_y2, torch.mul(pred_h2, CONSTANTS.c0))
    _208 = torch.stack([_204, _205, _206, _207], -1)
    pred_boxes2 = torch.to(_208, dtype=6, layout=0, device=torch.device("cuda"))
    _209 = ops.prim.NumToTensor(torch.size(deltas6, 0))
    _210 = int(_209)
    _211 = ops.prim.NumToTensor(torch.size(deltas6, 1))
    proposals_i5 = torch.reshape(pred_boxes2, [_210, int(_211)])
    proposals_i6 = torch.view(proposals_i5, [_56, -1, _175])
    B3 = ops.prim.NumToTensor(torch.size(_12, 1))
    _212 = int(B3)
    _213 = int(B3)
    deltas7 = torch.reshape(pred_anchor_deltas_i3, [-1, int(B3)])
    _214 = torch.expand(torch.unsqueeze(_12, 0), [_55, -1, -1])
    boxes7 = torch.reshape(_214, [-1, _213])
    deltas8 = torch.to(deltas7, 6)
    boxes8 = torch.to(boxes7, 6)
    _215 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    _216 = torch.select(_215, 1, 2)
    _217 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    widths3 = torch.sub(_216, torch.select(_217, 1, 0))
    _218 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    _219 = torch.select(_218, 1, 3)
    _220 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    heights3 = torch.sub(_219, torch.select(_220, 1, 1))
    _221 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    ctr_x3 = torch.add(torch.select(_221, 1, 0), torch.mul(widths3, CONSTANTS.c0))
    _222 = torch.slice(boxes8, 0, 0, 9223372036854775807)
    ctr_y3 = torch.add(torch.select(_222, 1, 1), torch.mul(heights3, CONSTANTS.c0))
    _223 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _224 = torch.slice(_223, 1, 0, 9223372036854775807, 4)
    dx3 = torch.div(_224, CONSTANTS.c1)
    _225 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _226 = torch.slice(_225, 1, 1, 9223372036854775807, 4)
    dy3 = torch.div(_226, CONSTANTS.c1)
    _227 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _228 = torch.slice(_227, 1, 2, 9223372036854775807, 4)
    dw7 = torch.div(_228, CONSTANTS.c1)
    _229 = torch.slice(deltas8, 0, 0, 9223372036854775807)
    _230 = torch.slice(_229, 1, 3, 9223372036854775807, 4)
    dh7 = torch.div(_230, CONSTANTS.c1)
    dw8 = torch.clamp(dw7, None, 4.1351665567423561)
    dh8 = torch.clamp(dh7, None, 4.1351665567423561)
    _231 = torch.slice(widths3, 0, 0, 9223372036854775807)
    _232 = torch.mul(dx3, torch.unsqueeze(_231, 1))
    _233 = torch.slice(ctr_x3, 0, 0, 9223372036854775807)
    pred_ctr_x3 = torch.add(_232, torch.unsqueeze(_233, 1))
    _234 = torch.slice(heights3, 0, 0, 9223372036854775807)
    _235 = torch.mul(dy3, torch.unsqueeze(_234, 1))
    _236 = torch.slice(ctr_y3, 0, 0, 9223372036854775807)
    pred_ctr_y3 = torch.add(_235, torch.unsqueeze(_236, 1))
    _237 = torch.exp(dw8)
    _238 = torch.slice(widths3, 0, 0, 9223372036854775807)
    pred_w3 = torch.mul(_237, torch.unsqueeze(_238, 1))
    _239 = torch.exp(dh8)
    _240 = torch.slice(heights3, 0, 0, 9223372036854775807)
    pred_h3 = torch.mul(_239, torch.unsqueeze(_240, 1))
    _241 = torch.sub(pred_ctr_x3, torch.mul(pred_w3, CONSTANTS.c0))
    _242 = torch.sub(pred_ctr_y3, torch.mul(pred_h3, CONSTANTS.c0))
    _243 = torch.add(pred_ctr_x3, torch.mul(pred_w3, CONSTANTS.c0))
    _244 = torch.add(pred_ctr_y3, torch.mul(pred_h3, CONSTANTS.c0))
    _245 = torch.stack([_241, _242, _243, _244], -1)
    pred_boxes3 = torch.to(_245, dtype=6, layout=0, device=torch.device("cuda"))
    _246 = ops.prim.NumToTensor(torch.size(deltas8, 0))
    _247 = int(_246)
    _248 = ops.prim.NumToTensor(torch.size(deltas8, 1))
    proposals_i7 = torch.reshape(pred_boxes3, [_247, int(_248)])
    proposals_i8 = torch.view(proposals_i7, [_54, -1, _212])
    _249 = torch.arange(1, dtype=None, layout=0, device=torch.device("cpu"), pin_memory=False)
    batch_idx = _0(_249, proposals_i0, )
    Hi_Wi_A = ops.prim.NumToTensor(torch.size(logits_i, 1))
    num_proposals_i = torch.clamp(Hi_Wi_A, None, 1000)
    _250 = int(num_proposals_i)
    _251, topk_idx = torch.topk(logits_i, int(num_proposals_i), 1)
    _252 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _253 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_252, 1), topk_idx])
    _254 = torch.index(proposals_i0, _253)
    _255 = torch.full([_250], 0, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _256 = _1(_255, proposals_i0, )
    Hi_Wi_A0 = ops.prim.NumToTensor(torch.size(logits_i0, 1))
    num_proposals_i0 = torch.clamp(Hi_Wi_A0, None, 1000)
    _257 = int(num_proposals_i0)
    _258, topk_idx0 = torch.topk(logits_i0, int(num_proposals_i0), 1)
    _259 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _260 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_259, 1), topk_idx0])
    _261 = torch.index(proposals_i2, _260)
    _262 = torch.full([_257], 1, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _263 = _2(_262, proposals_i0, )
    Hi_Wi_A1 = ops.prim.NumToTensor(torch.size(logits_i1, 1))
    num_proposals_i1 = torch.clamp(Hi_Wi_A1, None, 1000)
    _264 = int(num_proposals_i1)
    _265, topk_idx1 = torch.topk(logits_i1, int(num_proposals_i1), 1)
    _266 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _267 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_266, 1), topk_idx1])
    _268 = torch.index(proposals_i4, _267)
    _269 = torch.full([_264], 2, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _270 = _3(_269, proposals_i0, )
    Hi_Wi_A2 = ops.prim.NumToTensor(torch.size(logits_i2, 1))
    num_proposals_i2 = torch.clamp(Hi_Wi_A2, None, 1000)
    _271 = int(num_proposals_i2)
    _272, topk_idx2 = torch.topk(logits_i2, int(num_proposals_i2), 1)
    _273 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _274 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_273, 1), topk_idx2])
    _275 = torch.index(proposals_i6, _274)
    _276 = torch.full([_271], 3, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _277 = _4(_276, proposals_i0, )
    Hi_Wi_A3 = ops.prim.NumToTensor(torch.size(logits_i3, 1))
    num_proposals_i3 = torch.clamp(Hi_Wi_A3, None, 1000)
    _278 = int(num_proposals_i3)
    _279, topk_idx3 = torch.topk(logits_i3, int(num_proposals_i3), 1)
    _280 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _281 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_280, 1), topk_idx3])
    _282 = torch.index(proposals_i8, _281)
    _283 = torch.full([_278], 4, dtype=4, layout=0, device=torch.device("cpu"), pin_memory=False)
    _284 = _5(_283, proposals_i0, )
    topk_scores = torch.cat([_251, _258, _265, _272, _279], 1)
    topk_proposals = torch.cat([_254, _261, _268, _275, _282], 1)
    level_ids = torch.cat([_256, _263, _270, _277, _284])
    tensor = torch.select(topk_proposals, 0, 0)
    tensor0 = torch.to(tensor, 6)
    scores_per_img = torch.select(topk_scores, 0, 0)
    h, w, = torch.unbind(image_size)
    _285 = annotate(number, h)
    _286 = annotate(number, w)
    _287 = annotate(number, h)
    _288 = annotate(number, w)
    _289 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _290 = torch.clamp(torch.select(_289, 1, 0), 0, _288)
    _291 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _292 = torch.clamp(torch.select(_291, 1, 1), 0, _287)
    _293 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _294 = torch.clamp(torch.select(_293, 1, 2), 0, _286)
    _295 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _296 = torch.clamp(torch.select(_295, 1, 3), 0, _285)
    _297 = torch.stack([_290, _292, _294, _296], -1)
    box = torch.to(_297, dtype=6, layout=0, device=torch.device("cuda"))
    _298 = torch.slice(box, 0, 0, 9223372036854775807)
    _299 = torch.select(_298, 1, 2)
    _300 = torch.slice(box, 0, 0, 9223372036854775807)
    widths4 = torch.sub(_299, torch.select(_300, 1, 0))
    _301 = torch.slice(box, 0, 0, 9223372036854775807)
    _302 = torch.select(_301, 1, 3)
    _303 = torch.slice(box, 0, 0, 9223372036854775807)
    heights4 = torch.sub(_302, torch.select(_303, 1, 1))
    item = torch.__and__(torch.gt(widths4, 0.), torch.gt(heights4, 0.))
    _304 = annotate(List[Optional[Tensor]], [item])
    tensor1 = torch.index(box, _304)
    tensor2 = torch.to(tensor1, 6)
    _305 = annotate(List[Optional[Tensor]], [item])
    scores_per_img0 = torch.index(scores_per_img, _305)
    _306 = annotate(List[Optional[Tensor]], [item])
    _307 = torch.index(level_ids, _306)
    _308 = torch.to(tensor2, 6)
    keep = _6(_308, scores_per_img0, _307, 0.69999999999999996, )
    item0 = torch.slice(keep, 0, 0, 1000)
    _309 = annotate(List[Optional[Tensor]], [item0])
    tensor3 = torch.index(_308, _309)
    return torch.to(tensor3, 6)

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head, type=StandardRPNHead:
class StandardRPNHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d
  objectness_logits : __torch__.torch.nn.modules.conv.Conv2d
  anchor_deltas : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.StandardRPNHead,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    anchor_deltas = self.anchor_deltas
    objectness_logits = self.objectness_logits
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (objectness_logits).forward(_0, )
    _2 = (anchor_deltas).forward(_0, )
    _3 = (conv).forward1(argument_2, )
    _4 = (objectness_logits).forward1(_3, )
    _5 = (anchor_deltas).forward1(_3, )
    _6 = (conv).forward2(argument_3, )
    _7 = (objectness_logits).forward2(_6, )
    _8 = (anchor_deltas).forward2(_6, )
    _9 = (conv).forward3(argument_4, )
    _10 = (objectness_logits).forward3(_9, )
    _11 = (anchor_deltas).forward3(_9, )
    _12 = (conv).forward4(argument_5, )
    _13 = (_1, _4, _7, _10, (objectness_logits).forward4(_12, ), _2, _5, _8, _11, (anchor_deltas).forward4(_12, ))
    return _13

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward(input, )
  def forward1(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward1(input, )
  def forward2(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward2(input, )
  def forward3(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward3(input, )
  def forward4(self: __torch__.detectron2.layers.wrappers.___torch_mangle_130.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward4(input, )

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.conv.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward1(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward2(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward3(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)
  def forward4(self: __torch__.torch.nn.modules.activation.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.objectness_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    score = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward1(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    score = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward2(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    score = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward3(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    score = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score
  def forward4(self: __torch__.torch.nn.modules.conv.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    score = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return score

--------------------------------------------------------------------------------
Code for .model.proposal_generator.rpn_head.anchor_deltas, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward1(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward2(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward3(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x
  def forward4(self: __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    x = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return x

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.BufferList
  def forward(self: __torch__.detectron2.modeling.anchor_generator.DefaultAnchorGenerator,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    argument_6: Tensor,
    argument_7: Tensor,
    argument_8: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    _2 = __torch__.detectron2.layers.wrappers.move_device_like
    _3 = __torch__.detectron2.layers.wrappers.move_device_like
    _4 = __torch__.detectron2.layers.wrappers.move_device_like
    _5 = __torch__.detectron2.layers.wrappers.move_device_like
    _6 = __torch__.detectron2.layers.wrappers.move_device_like
    _7 = __torch__.detectron2.layers.wrappers.move_device_like
    _8 = __torch__.detectron2.layers.wrappers.move_device_like
    _9 = __torch__.detectron2.layers.wrappers.move_device_like
    cell_anchors = self.cell_anchors
    _40 = getattr(cell_anchors, "4")
    cell_anchors0 = self.cell_anchors
    _30 = getattr(cell_anchors0, "3")
    cell_anchors1 = self.cell_anchors
    _20 = getattr(cell_anchors1, "2")
    cell_anchors2 = self.cell_anchors
    _10 = getattr(cell_anchors2, "1")
    cell_anchors3 = self.cell_anchors
    _00 = getattr(cell_anchors3, "0")
    grid_height = ops.prim.NumToTensor(torch.size(argument_1, 2))
    grid_width = ops.prim.NumToTensor(torch.size(argument_1, 3))
    grid_height0 = ops.prim.NumToTensor(torch.size(argument_2, 2))
    grid_width0 = ops.prim.NumToTensor(torch.size(argument_2, 3))
    grid_height1 = ops.prim.NumToTensor(torch.size(argument_3, 2))
    grid_width1 = ops.prim.NumToTensor(torch.size(argument_3, 3))
    grid_height2 = ops.prim.NumToTensor(torch.size(argument_6, 2))
    grid_width2 = ops.prim.NumToTensor(torch.size(argument_7, 3))
    grid_height3 = ops.prim.NumToTensor(torch.size(argument_8, 2))
    grid_width3 = ops.prim.NumToTensor(torch.size(argument_8, 3))
    _11 = annotate(number, torch.mul(grid_width, CONSTANTS.c0))
    _12 = torch.arange(0., _11, 4, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _13 = _0(_12, _00, )
    _14 = torch.mul(grid_height, CONSTANTS.c0)
    _15 = torch.arange(0., annotate(number, _14), 4, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _16 = torch.meshgrid([_1(_15, _00, ), _13])
    shift_y, shift_x, = _16
    _17 = torch.reshape(shift_x, [-1])
    _18 = torch.reshape(shift_y, [-1])
    _19 = torch.stack([_17, _18, _17, _18], 1)
    shifts = torch.to(_19, dtype=6, layout=0, device=torch.device("cuda"))
    _21 = torch.add(torch.view(shifts, [-1, 1, 4]), torch.view(_00, [1, -1, 4]))
    tensor = torch.reshape(_21, [-1, 4])
    _22 = torch.mul(grid_width0, CONSTANTS.c1)
    _23 = torch.arange(0., annotate(number, _22), 8, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _24 = _2(_23, _10, )
    _25 = torch.mul(grid_height0, CONSTANTS.c1)
    _26 = torch.arange(0., annotate(number, _25), 8, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _27 = torch.meshgrid([_3(_26, _10, ), _24])
    shift_y0, shift_x0, = _27
    _28 = torch.reshape(shift_x0, [-1])
    _29 = torch.reshape(shift_y0, [-1])
    _31 = torch.stack([_28, _29, _28, _29], 1)
    shifts0 = torch.to(_31, dtype=6, layout=0, device=torch.device("cuda"))
    _32 = torch.add(torch.view(shifts0, [-1, 1, 4]), torch.view(_10, [1, -1, 4]))
    tensor0 = torch.reshape(_32, [-1, 4])
    _33 = torch.mul(grid_width1, CONSTANTS.c2)
    _34 = torch.arange(0., annotate(number, _33), 16, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _35 = _4(_34, _20, )
    _36 = torch.mul(grid_height1, CONSTANTS.c2)
    _37 = torch.arange(0., annotate(number, _36), 16, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _38 = torch.meshgrid([_5(_37, _20, ), _35])
    shift_y1, shift_x1, = _38
    _39 = torch.reshape(shift_x1, [-1])
    _41 = torch.reshape(shift_y1, [-1])
    _42 = torch.stack([_39, _41, _39, _41], 1)
    shifts1 = torch.to(_42, dtype=6, layout=0, device=torch.device("cuda"))
    _43 = torch.add(torch.view(shifts1, [-1, 1, 4]), torch.view(_20, [1, -1, 4]))
    tensor1 = torch.reshape(_43, [-1, 4])
    _44 = torch.mul(grid_width2, CONSTANTS.c3)
    _45 = torch.arange(0., annotate(number, _44), 32, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _46 = _6(_45, _30, )
    _47 = torch.mul(grid_height2, CONSTANTS.c3)
    _48 = torch.arange(0., annotate(number, _47), 32, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _49 = torch.meshgrid([_7(_48, _30, ), _46])
    shift_y2, shift_x2, = _49
    _50 = torch.reshape(shift_x2, [-1])
    _51 = torch.reshape(shift_y2, [-1])
    _52 = torch.stack([_50, _51, _50, _51], 1)
    shifts2 = torch.to(_52, dtype=6, layout=0, device=torch.device("cuda"))
    _53 = torch.add(torch.view(shifts2, [-1, 1, 4]), torch.view(_30, [1, -1, 4]))
    tensor2 = torch.reshape(_53, [-1, 4])
    _54 = torch.mul(grid_width3, CONSTANTS.c4)
    _55 = torch.arange(0., annotate(number, _54), 64, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _56 = _8(_55, _40, )
    _57 = torch.mul(grid_height3, CONSTANTS.c4)
    _58 = torch.arange(0., annotate(number, _57), 64, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False)
    _59 = torch.meshgrid([_9(_58, _40, ), _56])
    shift_y3, shift_x3, = _59
    _60 = torch.reshape(shift_x3, [-1])
    _61 = torch.reshape(shift_y3, [-1])
    _62 = torch.stack([_60, _61, _60, _61], 1)
    shifts3 = torch.to(_62, dtype=6, layout=0, device=torch.device("cuda"))
    _63 = torch.add(torch.view(shifts3, [-1, 1, 4]), torch.view(_40, [1, -1, 4]))
    tensor3 = torch.reshape(_63, [-1, 4])
    tensor4 = torch.to(tensor, 6)
    tensor5 = torch.to(tensor0, 6)
    tensor6 = torch.to(tensor1, 6)
    tensor7 = torch.to(tensor2, 6)
    tensor8 = torch.to(tensor3, 6)
    _64 = (tensor4, tensor5, tensor6, tensor7, tensor8)
    return _64

--------------------------------------------------------------------------------
Code for .model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", "1", "2", "3", "4", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  __annotations__["1"] = Tensor
  __annotations__["2"] = Tensor
  __annotations__["3"] = Tensor
  __annotations__["4"] = Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]

--------------------------------------------------------------------------------
Code for .model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  box_pooler : __torch__.detectron2.modeling.poolers.ROIPooler
  box_head : __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead
  box_predictor : __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers
  mask_pooler : __torch__.detectron2.modeling.poolers.___torch_mangle_145.ROIPooler
  mask_head : __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.StandardROIHeads,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor,
    image_size: Tensor,
    argument_7: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    mask_head = self.mask_head
    mask_pooler = self.mask_pooler
    box_predictor = self.box_predictor
    box_head = self.box_head
    box_pooler = self.box_pooler
    _1 = (box_pooler).forward(argument_1, argument_2, argument_3, argument_4, argument_5, )
    _2 = (box_predictor).forward((box_head).forward(_1, ), )
    _3, _4, = _2
    _5 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    _6 = int(_5)
    deltas = torch.to(_3, 6)
    boxes = torch.to(argument_2, 6)
    _7 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _8 = torch.select(_7, 1, 2)
    _9 = torch.slice(boxes, 0, 0, 9223372036854775807)
    widths = torch.sub(_8, torch.select(_9, 1, 0))
    _10 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _11 = torch.select(_10, 1, 3)
    _12 = torch.slice(boxes, 0, 0, 9223372036854775807)
    heights = torch.sub(_11, torch.select(_12, 1, 1))
    _13 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_13, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _14 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_14, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _15 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _16 = torch.slice(_15, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_16, CONSTANTS.c1)
    _17 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _18 = torch.slice(_17, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_18, CONSTANTS.c1)
    _19 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _20 = torch.slice(_19, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_20, CONSTANTS.c2)
    _21 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _22 = torch.slice(_21, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_22, CONSTANTS.c2)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _23 = torch.slice(widths, 0, 0, 9223372036854775807)
    _24 = torch.mul(dx, torch.unsqueeze(_23, 1))
    _25 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_24, torch.unsqueeze(_25, 1))
    _26 = torch.slice(heights, 0, 0, 9223372036854775807)
    _27 = torch.mul(dy, torch.unsqueeze(_26, 1))
    _28 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_27, torch.unsqueeze(_28, 1))
    _29 = torch.exp(dw0)
    _30 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_29, torch.unsqueeze(_30, 1))
    _31 = torch.exp(dh0)
    _32 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_31, torch.unsqueeze(_32, 1))
    _33 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    _34 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    _35 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    _36 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    _37 = torch.stack([_33, _34, _35, _36], -1)
    pred_boxes = torch.to(_37, dtype=6, layout=0, device=torch.device("cuda"))
    _38 = ops.prim.NumToTensor(torch.size(deltas, 0))
    _39 = int(_38)
    _40 = ops.prim.NumToTensor(torch.size(deltas, 1))
    _41 = torch.reshape(pred_boxes, [_39, int(_40)])
    boxes0, = torch.split_with_sizes(_41, [_6])
    _42 = ops.prim.NumToTensor(torch.size(boxes, 0))
    _43 = int(_42)
    _44 = torch.split_with_sizes(torch.softmax(_4, -1), [_43])
    scores, = _44
    _45 = torch.slice(scores, 0, 0, 9223372036854775807)
    scores0 = torch.slice(_45, 1, 0, -1)
    _46 = ops.prim.NumToTensor(torch.size(boxes0, 1))
    num_bbox_reg_classes = torch.div(_46, CONSTANTS.c3, rounding_mode="trunc")
    _47 = int(num_bbox_reg_classes)
    tensor = torch.reshape(boxes0, [-1, 4])
    tensor0 = torch.to(tensor, 6)
    h, w, = torch.unbind(image_size)
    _48 = annotate(number, h)
    _49 = annotate(number, w)
    _50 = annotate(number, h)
    _51 = annotate(number, w)
    _52 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _53 = torch.clamp(torch.select(_52, 1, 0), 0, _51)
    _54 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _55 = torch.clamp(torch.select(_54, 1, 1), 0, _50)
    _56 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _57 = torch.clamp(torch.select(_56, 1, 2), 0, _49)
    _58 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    _59 = torch.clamp(torch.select(_58, 1, 3), 0, _48)
    _60 = torch.stack([_53, _55, _57, _59], -1)
    _61 = torch.to(_60, dtype=6, layout=0, device=torch.device("cuda"))
    boxes1 = torch.view(_61, [-1, _47, 4])
    filter_mask = torch.gt(scores0, 0.050000000000000003)
    filter_inds = torch.nonzero(filter_mask)
    _62 = annotate(List[Optional[Tensor]], [filter_mask])
    boxes2 = torch.index(boxes1, _62)
    _63 = annotate(List[Optional[Tensor]], [filter_mask])
    scores1 = torch.index(scores0, _63)
    _64 = torch.slice(filter_inds, 0, 0, 9223372036854775807)
    _65 = torch.select(_64, 1, 1)
    boxes3 = torch.to(boxes2, 6)
    keep = _0(boxes3, scores1, _65, 0.5, )
    keep0 = torch.slice(keep, 0, 0, 100)
    _66 = annotate(List[Optional[Tensor]], [keep0])
    tensor1 = torch.index(boxes3, _66)
    _67 = annotate(List[Optional[Tensor]], [keep0])
    _68 = torch.index(scores1, _67)
    _69 = annotate(List[Optional[Tensor]], [keep0])
    filter_inds0 = torch.index(filter_inds, _69)
    tensor2 = torch.to(tensor1, 6)
    _70 = torch.slice(filter_inds0, 0, 0, 9223372036854775807)
    class_pred = torch.select(_70, 1, 1)
    _71 = (mask_pooler).forward(argument_1, tensor2, argument_3, argument_4, argument_7, )
    _72 = (mask_head).forward(_71, class_pred, tensor2, )
    return (tensor2, class_pred, _72, _68)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.ROIPooler,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    _1 = __torch__.detectron2.modeling.poolers._create_zeros
    level_poolers = self.level_poolers
    _3 = getattr(level_poolers, "3")
    level_poolers0 = self.level_poolers
    _2 = getattr(level_poolers0, "2")
    level_poolers1 = self.level_poolers
    _10 = getattr(level_poolers1, "1")
    level_poolers2 = self.level_poolers
    _00 = getattr(level_poolers2, "0")
    _4 = torch.cat([argument_2])
    _5 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    _6 = torch.to(torch.stack([_5]), dtype=4, layout=0, device=torch.device("cuda"))
    pooler_fmt_boxes = _0(_4, _6, )
    _7 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _8 = torch.select(_7, 1, 2)
    _9 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _11 = torch.sub(_8, torch.select(_9, 1, 0))
    _12 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _13 = torch.select(_12, 1, 3)
    _14 = torch.slice(argument_2, 0, 0, 9223372036854775807)
    _15 = torch.sub(_13, torch.select(_14, 1, 1))
    box_sizes = torch.sqrt(torch.mul(_11, _15))
    _16 = torch.add(torch.div(box_sizes, CONSTANTS.c0), CONSTANTS.c1)
    _17 = torch.add(torch.log2(_16), CONSTANTS.c2)
    level_assignments = torch.floor(_17)
    level_assignments0 = torch.clamp(level_assignments, 2, 5)
    level_assignments1 = torch.sub(torch.to(level_assignments0, 4), CONSTANTS.c3)
    output = _1(pooler_fmt_boxes, 256, 7, 7, argument_1, )
    x = torch.eq(level_assignments1, 0)
    inds, = torch.nonzero_numpy(x)
    _18 = annotate(List[Optional[Tensor]], [inds])
    rois = torch.index(pooler_fmt_boxes, _18)
    _19 = (_00).forward(rois, argument_1, )
    _20 = annotate(List[Optional[Tensor]], [inds])
    output0 = torch.index_put_(output, _20, _19)
    x0 = torch.eq(level_assignments1, 1)
    inds0, = torch.nonzero_numpy(x0)
    _21 = annotate(List[Optional[Tensor]], [inds0])
    rois0 = torch.index(pooler_fmt_boxes, _21)
    _22 = (_10).forward(rois0, argument_3, )
    _23 = annotate(List[Optional[Tensor]], [inds0])
    output1 = torch.index_put_(output0, _23, _22)
    x1 = torch.eq(level_assignments1, 2)
    inds1, = torch.nonzero_numpy(x1)
    _24 = annotate(List[Optional[Tensor]], [inds1])
    rois1 = torch.index(pooler_fmt_boxes, _24)
    _25 = (_2).forward(rois1, argument_4, )
    _26 = annotate(List[Optional[Tensor]], [inds1])
    output2 = torch.index_put_(output1, _26, _25)
    x2 = torch.eq(level_assignments1, 3)
    inds2, = torch.nonzero_numpy(x2)
    _27 = annotate(List[Optional[Tensor]], [inds2])
    rois2 = torch.index(pooler_fmt_boxes, _27)
    _28 = (_3).forward(rois2, argument_5, )
    _29 = annotate(List[Optional[Tensor]], [inds2])
    return torch.index_put_(output2, _29, _28)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.___torch_mangle_132.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.___torch_mangle_133.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.___torch_mangle_134.ROIAlign

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.25, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_132.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.125, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_133.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_134.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.03125, 7, 7, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head, type=FastRCNNConvFCHead:
class FastRCNNConvFCHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  flatten : __torch__.torch.nn.modules.flatten.Flatten
  fc1 : __torch__.torch.nn.modules.linear.Linear
  fc_relu1 : __torch__.torch.nn.modules.activation.___torch_mangle_135.ReLU
  fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_136.Linear
  fc_relu2 : __torch__.torch.nn.modules.activation.___torch_mangle_137.ReLU
  def forward(self: __torch__.detectron2.modeling.roi_heads.box_head.FastRCNNConvFCHead,
    argument_1: Tensor) -> Tensor:
    fc_relu2 = self.fc_relu2
    fc2 = self.fc2
    fc_relu1 = self.fc_relu1
    fc1 = self.fc1
    flatten = self.flatten
    _0 = (fc1).forward((flatten).forward(argument_1, ), )
    _1 = (fc2).forward((fc_relu1).forward(_0, ), )
    return (fc_relu2).forward(_1, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.flatten, type=Flatten:
class Flatten(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.flatten.Flatten,
    argument_1: Tensor) -> Tensor:
    return torch.flatten(argument_1, 1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc1, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.Linear,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    input = torch.linear(argument_1, weight, bias)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu1, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_135.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc2, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_136.Linear,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    input = torch.linear(argument_1, weight, bias)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_head.fc_relu2, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_137.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor, type=FastRCNNOutputLayers:
class FastRCNNOutputLayers(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cls_score : __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear
  bbox_pred : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear
  def forward(self: __torch__.detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers,
    argument_1: Tensor) -> Tuple[Tensor, Tensor]:
    bbox_pred = self.bbox_pred
    cls_score = self.cls_score
    _0 = (cls_score).forward(argument_1, )
    _1 = ((bbox_pred).forward(argument_1, ), _0)
    return _1

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_138.Linear,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    input = torch.linear(argument_1, weight, bias)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    deltas = torch.linear(argument_1, weight, bias)
    return deltas

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_144.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.___torch_mangle_145.ROIPooler,
    argument_1: Tensor,
    tensor: Tensor,
    argument_3: Tensor,
    argument_4: Tensor,
    argument_5: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    _1 = __torch__.detectron2.modeling.poolers._create_zeros
    level_poolers = self.level_poolers
    _3 = getattr(level_poolers, "3")
    level_poolers0 = self.level_poolers
    _2 = getattr(level_poolers0, "2")
    level_poolers1 = self.level_poolers
    _10 = getattr(level_poolers1, "1")
    level_poolers2 = self.level_poolers
    _00 = getattr(level_poolers2, "0")
    _4 = torch.cat([tensor])
    _5 = ops.prim.NumToTensor(torch.size(tensor, 0))
    _6 = torch.to(torch.stack([_5]), dtype=4, layout=0, device=torch.device("cuda"))
    pooler_fmt_boxes = _0(_4, _6, )
    _7 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _8 = torch.select(_7, 1, 2)
    _9 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _11 = torch.sub(_8, torch.select(_9, 1, 0))
    _12 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _13 = torch.select(_12, 1, 3)
    _14 = torch.slice(tensor, 0, 0, 9223372036854775807)
    _15 = torch.sub(_13, torch.select(_14, 1, 1))
    box_sizes = torch.sqrt(torch.mul(_11, _15))
    _16 = torch.add(torch.div(box_sizes, CONSTANTS.c0), CONSTANTS.c1)
    _17 = torch.add(torch.log2(_16), CONSTANTS.c2)
    level_assignments = torch.floor(_17)
    level_assignments0 = torch.clamp(level_assignments, 2, 5)
    level_assignments1 = torch.sub(torch.to(level_assignments0, 4), CONSTANTS.c3)
    output = _1(pooler_fmt_boxes, 256, 14, 14, argument_1, )
    x = torch.eq(level_assignments1, 0)
    inds, = torch.nonzero_numpy(x)
    _18 = annotate(List[Optional[Tensor]], [inds])
    rois = torch.index(pooler_fmt_boxes, _18)
    _19 = (_00).forward(rois, argument_1, )
    _20 = annotate(List[Optional[Tensor]], [inds])
    output0 = torch.index_put_(output, _20, _19)
    x0 = torch.eq(level_assignments1, 1)
    inds0, = torch.nonzero_numpy(x0)
    _21 = annotate(List[Optional[Tensor]], [inds0])
    rois0 = torch.index(pooler_fmt_boxes, _21)
    _22 = (_10).forward(rois0, argument_3, )
    _23 = annotate(List[Optional[Tensor]], [inds0])
    output1 = torch.index_put_(output0, _23, _22)
    x1 = torch.eq(level_assignments1, 2)
    inds1, = torch.nonzero_numpy(x1)
    _24 = annotate(List[Optional[Tensor]], [inds1])
    rois1 = torch.index(pooler_fmt_boxes, _24)
    _25 = (_2).forward(rois1, argument_4, )
    _26 = annotate(List[Optional[Tensor]], [inds1])
    output2 = torch.index_put_(output1, _26, _25)
    x2 = torch.eq(level_assignments1, 3)
    inds2, = torch.nonzero_numpy(x2)
    _27 = annotate(List[Optional[Tensor]], [inds2])
    rois2 = torch.index(pooler_fmt_boxes, _27)
    _28 = (_3).forward(rois2, argument_5, )
    _29 = annotate(List[Optional[Tensor]], [inds2])
    return torch.index_put_(output2, _29, _28)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.___torch_mangle_140.ROIAlign
  __annotations__["1"] = __torch__.detectron2.layers.roi_align.___torch_mangle_141.ROIAlign
  __annotations__["2"] = __torch__.detectron2.layers.roi_align.___torch_mangle_142.ROIAlign
  __annotations__["3"] = __torch__.detectron2.layers.roi_align.___torch_mangle_143.ROIAlign

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_140.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.25, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.1, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_141.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.125, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.2, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_142.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_pooler.level_poolers.3, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_143.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    _0 = ops.torchvision.roi_align(argument_2, boxes, 0.03125, 14, 14, 0, True)
    return _0

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head, type=MaskRCNNConvUpsampleHead:
class MaskRCNNConvUpsampleHead(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mask_fcn1 : __torch__.detectron2.layers.wrappers.___torch_mangle_147.Conv2d
  mask_fcn2 : __torch__.detectron2.layers.wrappers.___torch_mangle_149.Conv2d
  mask_fcn3 : __torch__.detectron2.layers.wrappers.___torch_mangle_151.Conv2d
  mask_fcn4 : __torch__.detectron2.layers.wrappers.___torch_mangle_153.Conv2d
  deconv : __torch__.torch.nn.modules.conv.ConvTranspose2d
  deconv_relu : __torch__.torch.nn.modules.activation.___torch_mangle_154.ReLU
  predictor : __torch__.detectron2.layers.wrappers.___torch_mangle_155.Conv2d
  def forward(self: __torch__.detectron2.modeling.roi_heads.mask_head.MaskRCNNConvUpsampleHead,
    argument_1: Tensor,
    class_pred: Tensor,
    tensor: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    predictor = self.predictor
    deconv_relu = self.deconv_relu
    deconv = self.deconv
    mask_fcn4 = self.mask_fcn4
    mask_fcn3 = self.mask_fcn3
    mask_fcn2 = self.mask_fcn2
    mask_fcn1 = self.mask_fcn1
    _1 = (mask_fcn2).forward((mask_fcn1).forward(argument_1, ), )
    _2 = (mask_fcn4).forward((mask_fcn3).forward(_1, ), )
    _3 = (deconv_relu).forward((deconv).forward(_2, ), )
    _4 = (predictor).forward(_3, )
    num_masks = ops.prim.NumToTensor(torch.size(_4, 0))
    _5 = torch.arange(annotate(number, num_masks), dtype=None, layout=0, device=torch.device("cpu"), pin_memory=False)
    indices = _0(_5, class_pred, )
    _6 = annotate(List[Optional[Tensor]], [indices, class_pred])
    _7 = torch.slice(torch.index(_4, _6), 0, 0, 9223372036854775807)
    _8 = torch.sigmoid(torch.unsqueeze(_7, 1))
    _9 = ops.prim.NumToTensor(torch.size(tensor, 0))
    _10 = torch.split_with_sizes(_8, [int(_9)])
    _11, = _10
    return _11

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_146.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_147.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn1.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_146.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_148.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_149.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn2.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_148.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_150.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_151.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn3.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_150.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation : __torch__.torch.nn.modules.activation.___torch_mangle_152.ReLU
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_153.Conv2d,
    argument_1: Tensor) -> Tensor:
    activation = self.activation
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)
    return (activation).forward(input, )

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.mask_fcn4.activation, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_152.ReLU,
    input: Tensor) -> Tensor:
    return torch.relu(input)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv, type=ConvTranspose2d:
class ConvTranspose2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.conv.ConvTranspose2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    input = torch._convolution(argument_1, weight, bias, [2, 2], [0, 0], [1, 1], True, [0, 0], 1, False, False, True, True)
    return input

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.deconv_relu, type=ReLU:
class ReLU(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_154.ReLU,
    argument_1: Tensor) -> Tensor:
    return torch.relu(argument_1)

--------------------------------------------------------------------------------
Code for .model.roi_heads.mask_head.predictor, type=Conv2d:
class Conv2d(Module):
  __parameters__ = ["weight", "bias", ]
  __buffers__ = []
  weight : Tensor
  bias : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.wrappers.___torch_mangle_155.Conv2d,
    argument_1: Tensor) -> Tensor:
    bias = self.bias
    weight = self.weight
    pred_mask_logits = torch._convolution(argument_1, weight, bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)
    return pred_mask_logits

--------------------------------------------------------------------------------